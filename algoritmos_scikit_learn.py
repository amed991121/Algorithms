# -*- coding: utf-8 -*-
"""Algoritmos_Scikit_Learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_kBacLqQzNy5GykAgSlDU2Y9XltDCEEx
"""

# Importar bibliotecas necesarias
import pandas as pd
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Paso 1: Cargar y preprocesar los datos
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
column_names = [
    "Class", "Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium",
    "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins",
    "Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline"
]
data = pd.read_csv(url, header=None, names=column_names)

# Separar características (X) y etiquetas (y)
X = data.drop("Class", axis=1)
y = data["Class"]

# Paso 2: Dividir los datos en entrenamiento (80%) y prueba (20%) con Hold-out
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Función para obtener el mejor modelo con K-Fold Cross-Validation
def get_best_model(classifier, X_train, y_train, k=5, stratified=False):
    if stratified:
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    else:
        kf = KFold(n_splits=k, shuffle=True, random_state=42)

    best_accuracy = 0
    best_model = None
    for train_index, val_index in kf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]
        model = classifier()
        model.fit(X_train_fold, y_train_fold)
        y_pred = model.predict(X_val_fold)
        accuracy = accuracy_score(y_val_fold, y_pred)
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = model
    return best_model, best_accuracy

# Función para evaluar un modelo en el conjunto de prueba
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    accuracy = report['accuracy']
    recall = {label: report[label]['recall'] for label in report if label.isdigit()}
    precision = {label: report[label]['precision'] for label in report if label.isdigit()}
    return accuracy, recall, precision

# Paso 3 y 4: Entrenar y evaluar con K-Fold Cross-Validation
print("### Resultados con K-Fold Cross-Validation ###")

# Árbol de Decisión
best_dt, best_dt_accuracy = get_best_model(DecisionTreeClassifier, X_train, y_train)
dt_accuracy, dt_recall, dt_precision = evaluate_model(best_dt, X_test, y_test)

# Naive-Bayes
best_nb, best_nb_accuracy = get_best_model(GaussianNB, X_train, y_train)
nb_accuracy, nb_recall, nb_precision = evaluate_model(best_nb, X_test, y_test)

# k-NN
best_knn, best_knn_accuracy = get_best_model(KNeighborsClassifier, X_train, y_train)
knn_accuracy, knn_recall, knn_precision = evaluate_model(best_knn, X_test, y_test)

# Imprimir resultados
print("\nÁrbol de Decisión:")
print(f"Exactitud: {dt_accuracy:.4f}")
print(f"Recall por clase: {dt_recall}")
print(f"Precisión por clase: {dt_precision}")

print("\nNaive-Bayes:")
print(f"Exactitud: {nb_accuracy:.4f}")
print(f"Recall por clase: {nb_recall}")
print(f"Precisión por clase: {nb_precision}")

print("\nk-NN:")
print(f"Exactitud: {knn_accuracy:.4f}")
print(f"Recall por clase: {knn_recall}")
print(f"Precisión por clase: {knn_precision}")

# Paso 5: Repetir el proceso con StratifiedKFold
print("\n### Resultados con StratifiedKFold ###")

# Árbol de Decisión con StratifiedKFold
best_dt_strat, best_dt_accuracy_strat = get_best_model(DecisionTreeClassifier, X_train, y_train, stratified=True)
dt_accuracy_strat, dt_recall_strat, dt_precision_strat = evaluate_model(best_dt_strat, X_test, y_test)

# Naive-Bayes con StratifiedKFold
best_nb_strat, best_nb_accuracy_strat = get_best_model(GaussianNB, X_train, y_train, stratified=True)
nb_accuracy_strat, nb_recall_strat, nb_precision_strat = evaluate_model(best_nb_strat, X_test, y_test)

# k-NN con StratifiedKFold
best_knn_strat, best_knn_accuracy_strat = get_best_model(KNeighborsClassifier, X_train, y_train, stratified=True)
knn_accuracy_strat, knn_recall_strat, knn_precision_strat = evaluate_model(best_knn_strat, X_test, y_test)

# Imprimir resultados con StratifiedKFold
print("\nÁrbol de Decisión:")
print(f"Exactitud: {dt_accuracy_strat:.4f}")
print(f"Recall por clase: {dt_recall_strat}")
print(f"Precisión por clase: {dt_precision_strat}")

print("\nNaive-Bayes:")
print(f"Exactitud: {nb_accuracy_strat:.4f}")
print(f"Recall por clase: {nb_recall_strat}")
print(f"Precisión por clase: {nb_precision_strat}")

print("\nk-NN:")
print(f"Exactitud: {knn_accuracy_strat:.4f}")
print(f"Recall por clase: {knn_recall_strat}")
print(f"Precisión por clase: {knn_precision_strat}")